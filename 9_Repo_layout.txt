Repo layout for your AML Agentic Platform with a Master agent + MCP servers. Itâ€™s designed to be minimal, modular, and extendable.

ðŸ“‚ Repo Structure
aml-agentic-platform/
â”‚
â”œâ”€â”€ master-agent/                # Orchestrator
â”‚   â”œâ”€â”€ main.py                  # Entry point
â”‚   â”œâ”€â”€ planner.py               # Task decomposition
â”‚   â”œâ”€â”€ executor.py              # Calls agents
â”‚   â”œâ”€â”€ verifier.py              # Verification logic
â”‚   â”œâ”€â”€ kb/                      # Knowledge base (graph store)
â”‚   â”‚   â”œâ”€â”€ kb_graph.json
â”‚   â”‚   â””â”€â”€ kb_utils.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mcp_databricks/              # MCP server for Databricks
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ dlt_create_update.py
â”‚   â”‚   â”œâ”€â”€ jobs_run.py
â”‚   â”‚   â””â”€â”€ uc_grant.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mcp_synapse_sql/             # MCP server for Synapse Serverless
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ exec_query.py
â”‚   â”‚   â”œâ”€â”€ create_view.py
â”‚   â”‚   â””â”€â”€ cetas.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mcp_mlflow/                  # MCP server for ML lifecycle
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ register.py
â”‚   â”‚   â”œâ”€â”€ promote.py
â”‚   â”‚   â””â”€â”€ batch_score.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mcp_sql_case_pbi/            # MCP server for Case Mgmt & BI
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ sql_exec.py
â”‚   â”‚   â”œâ”€â”€ schema_ensure.py
â”‚   â”‚   â”œâ”€â”€ pbi_refresh.py
â”‚   â”‚   â””â”€â”€ pbi_rbac_set.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md

ðŸ§  Master Agent (Orchestrator)
master-agent/main.py
import sys, json
from planner import plan_tasks
from executor import execute_plan
from kb.kb_utils import record_plan, record_result

def main():
    request = sys.argv[1] if len(sys.argv) > 1 else "Deploy baseline rules"
    plan = plan_tasks(request)
    record_plan(plan)

    for task in plan:
        result = execute_plan(task)
        record_result(task, result)

    print("âœ… All tasks executed successfully")

if __name__ == "__main__":
    main()
master-agent/planner.py (toy decomposition logic)
def plan_tasks(request: str):
    if "baseline rules" in request.lower():
        return [
            {
              "id": "T-1",
              "agent": "databricks",
              "tool": "dlt_create_update",
              "args": {
                  "catalog": "aml",
                  "target_schema": "silver",
                  "notebooks": [
                      "/Repos/aml/dlt/watchlists.py",
                      "/Repos/aml/dlt/rules_baseline.py"
                  ],
                  "continuous": True,
                  "storage": "abfss://logs@<sa>.dfs.core.windows.net/dlt/aml-core"
              }
            },
            {
              "id": "T-2",
              "agent": "synapse_sql",
              "tool": "create_view",
              "args": {
                  "sql": "CREATE OR ALTER VIEW vw_alerts AS SELECT * FROM OPENROWSET(...)" 
              }
            }
        ]
    return []
master-agent/executor.py
import subprocess, json

def execute_plan(task):
    agent = task["agent"]
    tool  = task["tool"]
    args  = json.dumps(task["args"])

    # Example: call MCP server via CLI
    cmd = ["mcp", agent, tool, args]   # placeholder, adapt to your MCP client
    print(f"â–¶ Running {agent}.{tool} ...")

    result = subprocess.run(cmd, capture_output=True, text=True)
    return {"task_id": task["id"], "stdout": result.stdout, "stderr": result.stderr}
master-agent/verifier.py
def verify_result(result):
    if "error" in result["stderr"].lower():
        return False
    return True

ðŸ›  Example MCP server (Databricks)
mcp_databricks/server.py
from mcp.server.fastmcp import FastMCPServer
from tools.dlt_create_update import dlt_create_update
from tools.jobs_run import jobs_run
from tools.uc_grant import uc_grant

server = FastMCPServer("databricks")

server.register_tool(dlt_create_update)
server.register_tool(jobs_run)
server.register_tool(uc_grant)

if __name__ == "__main__":
    server.run()
mcp_databricks/tools/dlt_create_update.py
from mcp.server.fastmcp import Tool

@Tool("dlt_create_update")
def dlt_create_update(catalog: str, target_schema: str, notebooks: list, continuous: bool, storage: str):
    """
    Create or update a DLT pipeline in Databricks.
    """
    # TODO: use databricks-cli or REST API
    return {"status":"ok", "pipeline_id":"pipeline-123", "tables":[f"{catalog}.{target_schema}.alerts"]}

ðŸ“– README.md
# AML Agentic Platform

This repo implements an **Agentic AML Platform** with:
- **Master Agent**: Plans, executes, verifies tasks.
- **MCP Servers**: Wrap Azure/Databricks/ML/SQL/PBI functions.

## Agents
- `mcp_databricks`: Databricks (DLT, Jobs, UC).
- `mcp_synapse_sql`: Synapse Serverless (views, queries).
- `mcp_mlflow`: ML lifecycle (train, register, score).
- `mcp_sql_case_pbi`: Case mgmt (SQL, Power BI refresh).

## Running
1. Start MCP servers:
   ```bash
   uv run mcp_databricks/server.py
   uv run mcp_synapse_sql/server.py
	â€¢	Run master agent:
	â€¢	uv run master-agent/main.py "Deploy baseline rules"
All tasks, results, and lineage are persisted in master-agent/kb/.

---

# ðŸ”œ Next Steps
- Fill in **real implementations** in each tool (`az cli`, `databricks-cli`, `pyodbc`, `mlflow`).  
- Wrap everything with your **`syntropAIkit.mcp.BaseSession`** (for retry, logging, safe builtins).  
- Add **gating/approval logic** in `master-agent/executor.py` before prod runs.  
- Connect to your **knowledge graph** so Master Agent reasons over current state.  

---

ðŸ‘‰ Do you want me to **generate working implementations** (e.g. Databricks API call for DLT 

